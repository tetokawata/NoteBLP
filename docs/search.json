[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "母平均の「補助線」の推定 (ver 0.1.2)",
    "section": "",
    "text": "Preface\nある属性 \\(X\\) を持つ事例内での、変数 \\(Y\\) の平均値 (“条件つき”平均)を推定する方法を紹介する、入門的なノートです1。 Rの実例では、 中古マンションの取引データを用いて、物件の属性 \\(X\\) (部屋の広さ、駅からの距離など)ごとに、平均取引価格 \\(Y\\) を推定します。\n平均の推定値は、さまざまな実務で活用されています。 中でも「\\(Y\\)の値を予測する」という課題において、中心的な役割を果たします。\n伝統的には、OLSが母平均の推定方法として用いられてきました。 近年では、OLSは母平均の仮想的な線型モデル(“補助線”)を推定する手法として、解釈できることが強調されています (Angrist and Pischke 2009; Aronow and Miller 2019) 。 モデルの定式に誤りがあったとしても、推定結果は常に”解釈”できることがその理由です。\nまた近年では、機械学習の手法も積極的に活用されています。 本ノートでは、機械学習の手法を導入する動機として、OLSが”研究者が設定した平均値のシンプルなモデル”を推定する方法であることを強調します。 シンプルなモデルを推定する限りは優れた方法ですが、より複雑なモデルを推定したい場合はその有効性を失います。 このような”複雑なモデル”を推定する方法として、LASSOを紹介します2。",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "母平均の「補助線」の推定 (ver 0.1.2)",
    "section": "",
    "text": "より専門的な入門としては、Ding (2024) などを参照してください。↩︎\n他の予測モデルの推定方法については、James et al. (2021) 参照↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "1  要約の基本コンセプト",
    "section": "",
    "text": "1.1 観察できない変数が引き起こす問題\nOLSやLASSOは、データが持つ特徴を要約するモデルを推定します。 要約は、データ分析における中核的アイディアであり、その重要性の理解が分析の第一歩となります。 まず本章では、要約の重要性を論じます。\n社会/市場分析における要約の必要性は、データから観察できない変数の存在にあります。 データから観察できない変数の存在は、あらゆる事例分析の最も深刻な問題の一つです。 このような変数への対処について、膨大な議論が蓄積されています。\n観察できない変数がもたらす問題は、個別事例分析において特に顕著です。 以下では、取引価格(Price; 単位 \\(=\\) 100万円) と物件の特徴を、事例分析から考察していきます 例えば、2億円で取引されている物件が、データの中に含まれていました。\nPrice\nSize\nLargeDistrict\n\n\n\n\n200\n105\n中心6区\nこの事例から、部屋の広さ(Size)が105平米で中心6区(港、中央、千代田、新宿、渋谷、文京)に立地し、2億円で取引された事例があることが確認できます。 では、この事例をもとに、同じ属性を持つ物件も、2億円で取引される傾向がある結論づけても良いでしょうか？ ほとんどの応用でこのような推論は、不適切です。\n同じデータの中に、取引価格以外について、全く同じ特徴を持つ物件の取引事例が、以下の3件ありました。 これらの事例と比較すると、2億円はかなり高い価格での取引だったことがわかります。\nPrice\nSize\nLargeDistrict\n\n\n\n\n200\n105\n中心6区\n\n\n150\n105\n中心6区\n\n\n92\n105\n中心6区\n\n\n110\n105\n中心6区\nなぜこのような取引価格のブレが生じるのでしょうか？ データの誤入力など潜在的な理由は複数ありますが、有力なのはこのデータに含まれない重要な変数 が存在することです。 例えば、町丁目、最寄駅や公園の近くに立地するか否かなど、より詳細な属性も、価格決定において重要であると予想されますが、このデータには含まれていません。 あるいは売り手や買い手の”交渉力”を反映している可能性もあります。 このように、多様な要因が取引価格に影響を与え、結果として取引価格の下振れ/上振れが生じます。\n観察できない変数は不動産のみならず、個人や家計、企業、あるいは国レベルの分析でも同様の問題を引き起こします。 観察できる変数 \\(X\\) が一致した事例内でも、観察できない変数は事例間で異る可能性が高く、結果 \\(Y\\) の値に大きな差が生まれます。\nそして現実の社会や市場の複雑さを考慮すると、どれだけ詳細な調査を行ったとしても、\\(Y\\)に影響を与える全ての要因を観察することは困難です。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>要約の基本コンセプト</span>"
    ]
  },
  {
    "objectID": "model.html#コンセプト-集計",
    "href": "model.html#コンセプト-集計",
    "title": "1  要約の基本コンセプト",
    "section": "1.2 コンセプト: 集計",
    "text": "1.2 コンセプト: 集計\n先の個別事例分析では、観察できない変数の偏りを確認する方法として、同じ\\(X\\)を持つ事例との整合性を確認しました。 このようなアプローチの発展として、同じ\\(X\\)を持つ事例集団について、\\(Y\\)の特徴を要約する方法があります。 例えば、平均値や分散、中央値、あるいは研究者による”所見”や”印象”、代表的な事例を紹介するなどです。\n恣意的な分析を避けるためには、調査計画を立てる時点で、要約方法も決定し、分析を通じてコミットすることが重要です。 このため分析内で、どのような「指標」を使用するのか、分析を開始する前に決定することが望まれます。\nよく用いられる指標は、平均値です。\n\n\n\n\n\n\nデータ上の平均値\n\n\n\n\\[\\frac{Y_1 + .. + Y_N}{N}\\]\nただし \\(Y_i\\) は第\\(i\\)事例の値、 \\(N\\) は事例数を表す。\n\n\n以下では、価格 (Price) と広さ (Size)について、データに含まれる事例の分布をHeat mapで図示しています。\n\n\n\n\n\n\n\n\n\n上記の散布図は、社会分析に用いるデータの持つ典型的な特徴を表しています。 極めて乱雑であり、同じ\\(X\\) でも \\(Y\\) が異なる事例が多くなっています。 これは、顕著な観察できない変数の影響を示唆しています。 また \\(X\\) の値に応じた事例数の偏りも大きく、特に100平米を超える/20平米を下回る物件の取引事例は少なくなっています。\n以下の各点は、部屋の広さごとに計算された平均値を図示しています。\n\n\n\n\n\n\n\n\n\n同図からは、 部屋が広くなると取引価格は高くなる傾向が読み取れます。\n多くの応用で、このような\\(X\\)ごとに集計するだけでは、現実の社会や市場の特徴について論じることは不適切です。 本ノートでは、以下の少数事例の問題の解決に注力します。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>要約の基本コンセプト</span>"
    ]
  },
  {
    "objectID": "model.html#少数事例の集計",
    "href": "model.html#少数事例の集計",
    "title": "1  要約の基本コンセプト",
    "section": "1.3 少数事例の集計",
    "text": "1.3 少数事例の集計\n平均値は有力な要約方法ですが、算出に使用する事例の数に注意してください。 部屋の広さ (Size) ごとの事例数は、以下の通りです。\n\n\n\n\n\n\n\n\nFigure 1.1\n\n\n\n\n\n特に20平米を下回る/100平米を超える物件について、事例が少なくなっており、5事例前後の組み合わせも散見されます。 このような小規模な事例数からの計算は、多くの問題が発生します。 そして、OLSやLASSOはそれに対応するための手法と解釈できることを強調します。\n以下、Chapter 2 では、事例数が少ないと、平均値も各事例の観察できない変数の偏りの影響を強く受ける可能性を指摘します。Chapter 3 と Chapter 5 では、このような小規模事例の集計問題を緩和するための手法として、OLSやLASSOを紹介します。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>要約の基本コンセプト</span>"
    ]
  },
  {
    "objectID": "population.html",
    "href": "population.html",
    "title": "2  母平均",
    "section": "",
    "text": "2.1 頻度論\n少数事例の集計が引き起こす問題を正確に理解するために、頻度論と呼ばれる概念的枠組みを導入します1。\nまず、「自分と同じ課題に取り組む他の研究者達」を想像してください。 この研究者達は、あなたと同じ社会を対象に同じ手法を用いて分析しています。 ただし共通のデータではなく、独立して収集した異なるデータを用いるとします。 このような独立した研究者達は、あなたと同じ分析結果に到達するでしょうか？\n簡単な理科の実験であれば、到達することは可能です。 例えば水は、「水に不純物を入れない」、「大気圧が通常」などの実験の手続きを守れば、「誰がやっても」おおよそ100度で沸騰します。 このため誰がやっても同じ得られる結果を、“科学的事実”として合意することができます。\n対して現実社会における多くの現象について、独立した研究者が同じ結論に到達することは困難です。 なぜならば、分析に用いる事例が異なるためです。 ほとんどの応用で、独立して収集したデータが、研究者間で完全一致する可能性はほぼゼロです。\nデータに含まれる事例の偏りは、結論の不一致をもたらします。 ある研究者には、公園に近い物件の取引事例ばかりが、“偶然”集まってくるかもしれません。 このような研究者のデータで計算された取引価格の平均値は、他の研究者と比べて、上振れる可能性が高くなります。 このため要約した値であったとしても、研究者間で同じ値に合意できません。\n人々が同じ結果を観察できない、という問題は深刻です。 「独立した個人や組織が同じ結果を観察できるので推定結果を事実として認定する」、という強力な枠組みが活用できないからです。 さらに言えば、他者の結果と乖離することが予想されるのであれば、自身が分析した結果を”信じる”合理的な理由も無くなります。\nこの問題に対処するためには、何らかの概念的な枠組みが必要となります。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>母平均</span>"
    ]
  },
  {
    "objectID": "population.html#頻度論",
    "href": "population.html#頻度論",
    "title": "2  母平均",
    "section": "",
    "text": "事例: 報道機関による世論調査\n\n\n\n分析結果の不一致の典型例として、報道機関による世論調査が挙げられます。 複数の機関による調査結果が、毎月公開されますが、その結果は各社で異なっています。 理由は複数考えられますが、最も単純なものは、調査対象となる回答者が異なるためです。 典型的な世論調査では、各社が独立して電話番号をランダムに発生させるなどの方法で、1000-2000名ほどの回答者を極力ランダムに選んでいます。 しかしながら、異なる調査に同じ人が回答する確率は、非常に低くく、必然的に調査結果も異なります。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>母平均</span>"
    ]
  },
  {
    "objectID": "population.html#コンセプト-母集団とサンプリング",
    "href": "population.html#コンセプト-母集団とサンプリング",
    "title": "2  母平均",
    "section": "2.2 コンセプト: 母集団とサンプリング",
    "text": "2.2 コンセプト: 母集団とサンプリング\n分析結果の不一致の問題を適切に論じるために、母集団とサンプリングという分析概念を導入します。 これらの概念によって、すべての研究者に共通した正答(推定対象; Estimand)と各々のデータから得られる回答(推定値; Estimates)を、分離することを可能にします。\n\n\n\n\n\n\n母集団とサンプリング\n\n\n\n以下を想定する\n\n私たちが手にしているデータは、母集団という無数の事例の集団から、選ばれた(サンプリングされた)事例から構成されている\n\n母集団の全ての事例を用いて算出された値を推定対象と呼ぶ\nサンプリングされたデータから算出された値を推定値と呼ぶ\n\n本ノートでは、事例は完全ランダムに選ばる (ランダムサンプリング) を想定します。\n\n\n\nすなわち、「私たちが得られる推定値は、ランダムに部分的な事例から得たものである」と想定します。 一部の事例しか活用できていないため、母集団におけるすべての事例から計算された推定対象とは一致しません。\n例えば日本における男女間家事分担格差の実態把握を行いたいとします。 この場合、母集団は日本の家計全体となります。 もし日本の家計全体における家事負担を把握できれば、推定対象は容易に回答可能です。 しかしながら私たちのデータは、家計全体の一部であり、そこから得られる推定値（例えばデータ上の平均的な家事分担）は、推定対象(日本全体の平均的な家事負担)とは異なります。 このため日本全体では「女性の方が男性よりも家事負担が大きい」としても、データに含まれる事例が偶然偏り、「夫の家事負担がより大きい」という推定値を誤って示す可能性があります。\n母集団として、仮想的な集団を想定することもできます。 例えば、あるコンビニのレジデータに、ある日の全ての来客者について、購入金額が全て記録されているとします。 この場合、現実の来客者全てが記録されているため、母集団は存在しない、あるいは母集団の全てを観察できていると考えることも可能です。 一方で、その日にコンビニを訪れた顧客は、潜在的な顧客の一部であると想定することもできます。 皆さんも、よく利用するコンビニであったとしても、毎日は利用しないのではないでしょうか？ この場合、あなたは母集団である潜在的な顧客には属しますが、実際のデータには記録されない (偶然コンビニを利用しなかった) 可能性があります。\n\n2.2.1 母平均\n母集団に対しては、データの要約と同様の議論が適用できます。 母集団においても、\\(X\\)が同じで合ったとしても、\\(Y\\) の値が異なることが想定されます。 このため代表的な\\(Y\\)の値について、論じることが現実的です。\n本ノートでは引き続き、平均値について論じていきます。 母集団における平均値を以後、母平均と呼んでいきます。\n\n\n\n\n\n\n母平均\n\n\n\n\\(Y\\) の母集団における平均値\n\n\nデータ上での平均値とは異なり、母平均を正確に知ることは不可能です。 研究者は母集団を直接観察できないためです。 ただしもし母集団が直接観察できるのであれば、すべての研究者は同じ母平均を計算するため、母平均の値について合意できます。\n合意可能だが、直接計算できない母平均を、手元にある限られたデータから推測することが、本ノートの中心的な挑戦となります。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>母平均</span>"
    ]
  },
  {
    "objectID": "population.html#sec-Sim",
    "href": "population.html#sec-Sim",
    "title": "2  母平均",
    "section": "2.3 数値例",
    "text": "2.3 数値例\n以上の概念を明確にするために、簡単な数値実験を行います。 今、4名の研究者が独立して20事例を集めたとします。 各事例について、取引価格 \\(Y\\) と 部屋の広さ \\(X\\) がデータから観察できるとします\n母分布は以下のように設定しています。\n\n部屋の広さは、\\(X\\in\\{30,35,40,..,80\\}\\) が同じ割合で存在\n取引価格は、広さが70-75平米の間、急上昇する\n\n以下の図は、4名の研究者が手にするデータと平均値を図示しています。\n\n\n\n\n\n\n\n\n\n平均値について、研究者間で大きな違いが見られます。\nこの図に母平均を上書きすると以下のようになります。 ただし図を簡略化するために、各事例の値は排除します。\n\n\n\n\n\n\n\n\n\n重要な点として、母平均とデータ上の平均は乖離していることを確認してください。 また乖離の仕方は、研究者によって異なります。 言い換えるならば、推定対象である母平均は全員共通である一方で、推定値であるデータ上の平均値は異なっています。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>母平均</span>"
    ]
  },
  {
    "objectID": "population.html#大数の法則",
    "href": "population.html#大数の法則",
    "title": "2  母平均",
    "section": "2.4 大数の法則",
    "text": "2.4 大数の法則\n母平均を知る方法は、各\\(X\\)の組み合わせについて、ランダムサンプリングされた無限大の事例数で平均値を計算することです。 これは大数の法則が成立し、データ上の平均値 (推定値) と母平均 (推定対象) が一致するためです。\n\n\n\n\n\n\n一致性\n\n\n\n無限大の事例数をもつランダムサンプリングデータにおいて、計算された\\(Y\\)の平均値は、母平均と一致する。\n\n\n以下では、事例数を20,200,2000,20000に随時変更した数値例を示しています。\n\n\n\n\n\n\n\n\n\n20事例では、母平均とデータ上の平均値が大きく乖離してますが、20000事例ではほぼ一致していることが確認できます。\n一致性は重要な理論的性質ですが、実践上の含意は限られています。 多くの応用において、大量の\\(X\\)の組み合わせが発生する一方で、事例数は限られています。 このため十分な事例数を確保することができず、小規模事例の集計の問題と向き合う必要があります。\n\n2.4.1 信頼区間\n一致性は、データ上の平均値と母平均が一致を、無限大の事例数においてのみ保証します。 現実の事例数では、データ上の平均値と母平均は一致せず、研究者間でのばらつきも残ってしまいます。\n実際のデータ分析では、推定結果は信頼区間と共に示すことが一般的です。 信頼区間とは、一定の確率2 で、母平均を含む区間です。\n\n\n\n\n\n\n定義: 信頼区間\n\n\n\n一定の確率 (初期値では\\(95\\%\\)) で、母平均を含む区間\n\n\n例えば200事例を用いて計算した平均値の信頼区間は以下です。\n\n\n\n\n\n\n\n\n\n点で推定値、横線で95 \\(\\%\\) 信頼区間、縦線が母平均を示しています。 本数値例では、4名の研究者全員が、母平均を含む信頼区間を獲得できています。\n同じ数値例を、“100名の研究者”分実施した結果は以下です。\n\n\n\n\n\n\n\n\n\n縦軸は研究者の名前 (ID) を示しています。 赤字は、「不幸にも」母平均を含まない信頼区間が推定されてしまった研究者を示しています。 100名中6名 (概ね\\(95\\%\\))は、このような不幸に見舞われており、信頼区間の想定と整合的です。\n実際の応用では、研究者自身は、自分が運の悪い \\(5\\%\\) となるか、それとも幸運な \\(95\\%\\) になるのかは分かりません。 このため「\\(5\\%\\) のリスクは低い」として、自身が計算した信頼区間の中に真の値が”ほぼ”含まれているとして、結論を論じます。\nこのリスクを変更することは容易です。たとえばリスクを\\(0.5\\%\\)に変更した場合の\\(99.5\\%\\) 信頼区間は以下です。\n\n\n\n\n\n\n\n\n\n「不幸な」研究者は100名中1名のみとなりましたが、その代償として信頼区間が拡大しています。 すなわち結論を不明確にする代わりに、ミスリードな主張をするリスクを減らしています。\n「不幸な」研究者が発生する確率を\\(0\\%\\) にすることは一般に不可能です。 「\\(100\\%\\) 信頼区間」を計算しようとすると、区間の幅は無限大になります。\n信頼区間を正確に推定することは、一般に極めて困難です。 このため多くの実践で、近似的な信頼区間を算出し、報告します。 この近似的な算出は、事例数が「十分にある」ことを前提としています。 多くの数値実験から、具体的な事例数として150事例以上を要求されています3。\n部屋の広さと取引価格の例 (Figure 1.1) に戻ると、中間的な広さの物件については概ね200事例以上が確保されていますが、極端に小規模/大規模な物件については、事例数が50を大きく下回っています。 このため信頼区間の計算が困難です。\n\n\n\n\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis. 2024. “Applied Causal Inference Powered by ML and AI.” arXiv Preprint arXiv:2403.02467.\n\n\nLin, Hanti. 2024. “To Be a Frequentist or Bayesian? Five Positions in a Spectrum.” Harvard Data Science Review 6 (3).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>母平均</span>"
    ]
  },
  {
    "objectID": "population.html#footnotes",
    "href": "population.html#footnotes",
    "title": "2  母平均",
    "section": "",
    "text": "頻度論以外にはベイズ法と呼ばれる枠組みもあります。Lin (2024)などを参照ください。↩︎\n多くの統計言語で、初期値では\\(95\\%\\)が設定されています。↩︎\n事例数と平均値、信頼区間の詳細な関係性については、Chernozhukov et al. (2024) の1.2章と1.A章などを参照ください↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>母平均</span>"
    ]
  },
  {
    "objectID": "OLS.html",
    "href": "OLS.html",
    "title": "3  データ上でのOLS",
    "section": "",
    "text": "3.1 線型モデル\n少数事例の要約を避けるためには、より”大雑把な”要約が必要となります。 大雑把な要約の代表例として、線型モデルを紹介します。\n線型モデルは、手元のデータから\\(Y\\)の平均値が持つ性質を簡便に捉えるモデルであり、現代のデータ分析でも頻繁に利用されます。\n線型モデルを推定する方法としては、本章で最小二乗法(OLS)および@sec-LASSO でLASSOを紹介します。\nOLSによる推定は、研究者によるモデルの単純化が求められます。 適切な単純化がなされるのであれば、限られた事例数のもとでも、母平均の特徴を類推する有効な方法となり得ます (Chapter 4)。\n\\(Y\\) の平均値と \\(X_1,..,X_L\\) の関係性を記述するモデルを導入します。\n以下では\\(\\beta_0,..,\\beta_L\\)を決定する具体的な方法として、OLSを紹介します。\n線型モデルをどのように解釈すれば良いでしょうか？ 最も実践的な解釈は、平均値の”補助線”として捉えることです。\n以下の図では、Priceの平均値とSizeの関係性を捉えるための3つの”補助線”を書き込みます。 \\(\\beta_0 + \\beta_1\\times Size\\)の、パラメタの値のみ変更しています。\nデータ上の平均値は紫の点で示しています。 赤線は \\(0.05 + 0.7\\times Size\\)、 緑線は \\(30 + 0.5\\times Size\\)を示しています。\n水色線は\\(\\beta_0=80,\\beta_1=0\\) とした水平な「補助線」を示しています。\n赤線と緑線は、平均取引価格が持つ「Sizeとともに上昇する傾向がある」特徴をある程度捉えています。 対して水色線は、このような特徴を捉えられておらず、不適切であると考えれます。 モデルの大枠が同じでも、パラメタ \\(\\beta\\) の値によって、適切な要約か否かが決まってきます。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>データ上でのOLS</span>"
    ]
  },
  {
    "objectID": "OLS.html#線型モデル",
    "href": "OLS.html#線型モデル",
    "title": "3  データ上でのOLS",
    "section": "",
    "text": "線型モデル\n\n\n\n\\[Yの平均値\\simeq Yのモデル=\\beta_0 + \\beta_1X_1 + \\beta_2X_2 +.. + \\beta_LX_L\\]\n\n\\(\\beta_0,..,\\beta_L\\) はパラメタと呼ぶ",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>データ上でのOLS</span>"
    ]
  },
  {
    "objectID": "OLS.html#ols",
    "href": "OLS.html#ols",
    "title": "3  データ上でのOLS",
    "section": "3.2 OLS",
    "text": "3.2 OLS\nパラメタの値は、データに基づいて決定されることが通常です。 代表的な決定方法としては、最小二乗法 (OLS) が挙げられます。\n\n\n\n\n\n\nOLSの定義\n\n\n\n\n研究者が予測モデルの大枠を以下のように設定する \\[Yのモデル=\\beta_0 + \\beta_1X_1 + \\beta_2X_2 +.. + \\beta_LX_L\\]\n以下を最小化するように \\(\\beta_0,..,\\beta_L\\) を決定する \\[(Y-Yのモデル)^2のデータ上の平均\\]\n\n\n\nOLSは、研究者が事前に大枠を設定したモデルを、データに最も適合するように推定する手法であると解釈できます。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>データ上でのOLS</span>"
    ]
  },
  {
    "objectID": "OLS.html#sec-OLS_Example",
    "href": "OLS.html#sec-OLS_Example",
    "title": "3  データ上でのOLS",
    "section": "3.3 実例",
    "text": "3.3 実例\n\n3.3.1 単回帰\n２種類の\\(X\\) (Sizeと立地(中心6区か否か))について、取引価格の平均値を計算しました。\n\n\n\n\n\n\n\n\n\n左側のパネルは中心６区、右側は他の区について、各Sizeごとに平均取引価格を計算しています。\n平均値の最もシンプルな線型モデルとして、以下を推定してみます。 \\[モデル = \\beta_0 + \\beta_1\\times Size\\] \\(\\beta_0,\\beta_1\\) は、以下のデータ上の平均二乗誤差を最小化するように推定します。\\[(Y - モデル)^2 のデータ上の平均\\] このような推定方法は、単回帰として教科書では紹介されてきました。\n推定結果を図示すると、以下となります。\n\n\n\n\n\n\n\n\n\n広い物件は取引価格が高くなる傾向を捉えることができています。 しかしながら立地に関わらず同じモデルを当てはめており、中心６区の方が取引価格が高い傾向を捉えられていません。\n\n\n3.3.2 重回帰\n立地と平均取引価格の関係性を捉えるために、以下のモデルの推定を試みます。 \\[モデル = \\beta_0 + \\beta_1\\times Size + \\beta_2\\times District\\] \\(District\\) は、中心６区に立地していれば1、それ以外では0を取ります。 \\(\\beta_0,..,\\beta_2\\) は引き続き、データへの適合度を最大化するように推定できます。 このような推定方法は、重回帰として教科書では紹介されてきました。\n推定結果を図示すると、以下となります。\n\n\n\n\n\n\n\n\n\n中心6区の方が平均取引価格が高いという性質を上手く捉えています。 しかしながら、中心6区において広い物件の取引価格が一段と上昇するという性質は捉えきれていません。\n\n\n3.3.3 交差項と高次項の導入\n母平均が持つ複雑な性質を捉えるために、交差効果と高次項を導入し、さらに複雑なモデルを推定してみます。 \\[モデル = \\beta_0 + \\beta_1 Size+\\beta_7District + \\underbrace{\\beta_2Size^2 +..+\\beta_6Size^6}_{高次項}\\] \\[+\\underbrace{\\beta_8 Size\\times District +..+\\beta_{14}Size^6\\times District}_{交差効果}\\] このような複雑なモデルであったとしても、データへの適合度を最大化するように推定できます。\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 複雑なモデルの弊害\nより複雑なモデルを最小二乗法で推定すると、データへの適合度が改善し、モデルをデータ上の平均値により近づけることができます。 例えば、以下の図ではSizeの10乗まで加えた推定を行なっています。\n\n\n\n\n\n\n\n\n\nこのモデルでは、特に中心６区外に立地する物件について、ほぼほぼデータ上の平均値を近似するモデルが推定されています。 さらにモデルを複雑化すると、データ上の平均値を”なぞる”モデルが推定されます。\nしかしながら、母集団の特徴を捉えることを目標とするのであれば、このことは必ずしも望ましいとはいえません。 いうまでもなく、平均値をなぞるモデルは、単なる平均値とよく似た性質を持ちます。 このため、Chapter 1 で議論した少数事例の集計の問題を引き起こしてしまいます。\n以上の問題は、過剰適合/過学習の問題と呼ばれています。\n\n\n\n\n\n\n過剰適合/過学習\n\n\n\n複雑なモデルを、少ない事例数で推定した結果、データへの当てはまりは高くなるが、母平均からは乖離する現象",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>データ上でのOLS</span>"
    ]
  },
  {
    "objectID": "OLS.html#rによる実践例",
    "href": "OLS.html#rによる実践例",
    "title": "3  データ上でのOLS",
    "section": "3.4 Rによる実践例",
    "text": "3.4 Rによる実践例\n\n以下のパッケージを使用\n\nreadr (tidyverseに同梱): データの読み込み\n\n\nデータを取得します。\n\nData = readr::read_csv(\"Public.csv\") # データ読み込み\n\nlm関数を用いてOLSを推定します。\n\nOLS = lm(Price ~ Size + Tenure + StationDistance, # Y ~ X\n         Data # 使用するデータの指定\n         ) # OLS\n\nOLS\n\n\nCall:\nlm(formula = Price ~ Size + Tenure + StationDistance, data = Data)\n\nCoefficients:\n    (Intercept)             Size           Tenure  StationDistance  \n        19.7206           1.0199          -0.6392          -1.3851  \n\n\nCoefficientsが\\(\\beta\\)の推定値を示しています。 例えば、推定された線型モデルにおいて、Size(部屋の広さ)と平均取引価格は正の関係性がありますが、Tenure(築年数)とStationDistance(駅からの距離)は負の関係性が見られます。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>データ上でのOLS</span>"
    ]
  },
  {
    "objectID": "PopulationOLS.html",
    "href": "PopulationOLS.html",
    "title": "4  母集団上でのOLS",
    "section": "",
    "text": "4.1 Population OLS\nChapter 2 では、データ上の平均値と母平均との関係性について論じました。 データ上で実施したOLSは、平均値と同様に、母集団の特徴ついて何らかの含意を持つでしょうか？\n伝統的な教科書では、研究者が設定した線型モデルが、母平均の妥当なモデルであれば、OLSにより推定された線型モデルは、母平均の優れた推定値であることが強調されます。 しかしながら社会科学のほぼ全ての応用において、研究者が設定するモデルは誤定式化 (misspecification) を犯しており、どのように推定しても母平均から乖離していると想定すべきです。\n本章では、誤定式化を犯しているケースにおいても、OLSは母集団の特徴について明確な示唆を持つこと (Angrist and Pischke 2009; Aronow and Miller 2019) を紹介します。 また誤定式化を減らすためには、モデルを複雑化する必要がありますが、パラメタ推定の「精度」との間にトレードオフが生じることも強調します。\nこのことが持つ示唆は重要です。 経済学などの社会科学における実証研究は、しばしば推定するモデルが単純すぎるという批判を受けてきました。 これは的を射た批判であり、現実社会の複雑さに比べると、如何なるモデルも単純すぎると想定すべきです。 しかしながら、モデルの推定に用いることができる事例数に限りがあることも、同時に考慮する必要があります。 現実に合わした極めて複雑なモデルを推定しようとすると、そのパラメタの推定精度が大幅に低下してしまいます。 限られたデータと現実の複雑性の間で、適切な落とし所を見つけることが重要となります。\nOLSの推定結果は、母集団上での仮想的なOLS (Population OLS) の結果を推定していると解釈することができます。 少しわかりにくい考え方なので、Chapter 2 における数値例とともに確認します。\n仮想的な4名の研究者が、同じ母集団の特徴を解明しようとしているとします。 母平均は、全研究者共通で、以下の通りとなります。\nもし母集団上で、モデル \\(f(X)=\\beta_0 + \\beta_1X\\) をOLS推定できれば、以下の推定結果を得ることができます。\nPopulation OLSの結果は実線、母平均は点線で表しています。 以下の要点に特に注意してください。\n二点目は誤定式化の問題と呼ばれ、近年大きな議論がなされてきました。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>母集団上でのOLS</span>"
    ]
  },
  {
    "objectID": "PopulationOLS.html#population-ols",
    "href": "PopulationOLS.html#population-ols",
    "title": "4  母集団上でのOLS",
    "section": "",
    "text": "Population OLSは、同じ”データ”(母集団)を用いて推定しているので、全ての研究者が同じ推定結果となります。\n母平均とは必ずしも一致しません。本例において、母平均は70~75平米にかけて、平均取引価格が急上昇しています。しかしながら、“一直線”のモデルを推定しているため、このような傾向はモデルに反映されません。\n\n\n\n\n\n\n\n\n誤定式化の定義\n\n\n\nある線型モデル \\(\\beta_0+\\beta_1X_1+..+\\beta_LX_L\\) について、(\\(X_1..X_L\\) における)\\(Y\\)の母平均 と モデルを一致させるような \\(\\beta_0..\\beta_L\\) は存在しない",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>母集団上でのOLS</span>"
    ]
  },
  {
    "objectID": "PopulationOLS.html#ols-population-olsの推定",
    "href": "PopulationOLS.html#ols-population-olsの推定",
    "title": "4  母集団上でのOLS",
    "section": "4.2 OLS \\(=\\) Population OLSの推定",
    "text": "4.2 OLS \\(=\\) Population OLSの推定\n現実において実行可能な推定は、母集団ではなく、そこからランダムに選ばれた事例を用いたOLSです。 ここでは50事例を収集したとします。 各研究者は独自にデータ収集を行うため、データ上で行うOLSの結果には違いが生じることに注意してください。\n\n\n\n\n\n\n\n\n\n母平均は青の実線、Population OLSは青の点線、データ上でのOLSの結果は赤の実線で示しています。\nデータ上のOLS推定から得られるモデルは、母集団上でのOLSと一致はしていませんが、かなり近い性質を持っています。 事例数を増やすとさらに近くなることも確認できます。 以下では5000事例まで増やしています。\n\n\n\n\n\n\n\n\n\n全ての研究者について、Population OLSとデータ上でのOLSの乖離(赤と青の実践の乖離)は、“目視”できないほど小さくなっています。\nPopulation OLSとデータ上でのOLSは、母平均とデータ上での平均値と類似した理論的関係性を持ちます。\n\n\n\n\n\n\n性質: 大表本性質\n\n\n\n\n一致性: 事例数が無限大に大きくなると、データ上でのOLSはPopulation OLSの結果と一致する。\n信頼区間: 事例数がパラメタの数に比べて十分多いと、Population OLSの結果についての信頼区間を近似計算できる。\n\n\n\n上記の性質はあくまで、データ上でのOLSとPopulation OLSとの関係性を論じていることに、改めて注意してください。 大標本理論は、データ上でのOLSをPopulation OLSの推定値とみなすことを正当化します。\nしかしながら、母平均の推定値とみなすためには、誤定式化がないことが前提となります。 なぜならば誤定式化が存在すると、Population OLSと母平均は一致しないので、データ上でのOLSも母平均と一致することはあり得ません。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>母集団上でのOLS</span>"
    ]
  },
  {
    "objectID": "PopulationOLS.html#モデルの複雑化",
    "href": "PopulationOLS.html#モデルの複雑化",
    "title": "4  母集団上でのOLS",
    "section": "4.3 モデルの複雑化",
    "text": "4.3 モデルの複雑化\n実際の応用では、あらゆるモデルは誤定式化を犯している、少なくともその可能性を排除できない、と想定すべきです。 特に社会現象や個人行動の大部分はBlack Boxであり、信頼できるモデルを想定することは、事実上不可能です。 ただし誤定式化による母平均とPopulation OLSの乖離を削減することは容易です。 しかしながら、その代償として、Population OLSとデータ上でのOLSの乖離が大きくなる可能性があります。\nモデル \\(f(X)=\\beta_0 + \\beta_1X\\) に変わって、 \\(f(X)=\\beta_0 + \\beta_1X + \\beta_2X^2\\) を推定してみます。 仮想的なPopulation OLSの結果は以下です。\n\n\n\n\n\n\n\n\n\n\\(f(X)=\\beta_0 + \\beta_1X\\) のPopulation OLSの結果よりも、母平均に近づいていることが確認できます。 さらに\\(X\\)の8乗まで加えると (\\(f(X)=\\beta_0 + \\beta_1X +.. + \\beta_8X^8\\))、Population OLSは母平均をほぼ近似できることが確認できます。\n\n\n\n\n\n\n\n\n\n一般にモデルを複雑化する (\\(\\beta\\) の数を増やす)と、Population OLSと母平均は必ず近づきます1。\nもし母平均の推定が目的なのであれば、極力複雑なモデルを推定すべきでしょうか？ ここで注意が必要なのは、Population OLSは実際には実行できず、事例数が限られたデータ上でのOLSのみが可能なことです。\n以下では200事例のデータについて、\\(f(X)=\\beta_0 + \\beta_1X\\ (Y\\sim X)\\) および \\(f(X)=\\beta_0 + \\beta_1X_1 +.. + \\beta_8X^8\\) をOLS推定しました。\n\n\n\n\n\n\n\n\n\n母平均は青の点線で示しています。 データ上でのOLSの結果は、複雑なモデルについては赤の点線、シンプルなモデルについては赤の実線で示しています。\n複雑なモデルを推定した結果、単純なモデルよりも、母平均から大きく乖離した箇所が散見されます。 さらに複雑なモデルについての推定結果は、研究者間で大きなばらつきが見られます。\nこれはモデルを複雑化した結果、データ上の平均値に近づくことに原因があります (Section 3.3)。 限られた事例数のもとでは、小規模な事例のみから計算された平均値が発生します。 このような小規模集計を避けるために、線型モデルを活用した集計を行います。 しかしながらモデルを複雑化すると、この集計が再び上手くいかなくなります。 複雑なモデルをOLSで推定すると、小規模な事例の偶然の偏りを強く反映してしまい、結果母平均から大きく乖離してしまうリスクが高くなります。\nこの問題は、大規模事例が活用できる場合は、発生しにくくなります。 例えば20000事例について、OLS推定を行った結果は以下です。\n\n\n\n\n\n\n\n\n\n母平均は青の点線で示しています。 データ上でのOLSの結果は、複雑なモデルについては赤の点線、シンプルなモデルについては赤の実線で示しています。\n複雑なモデルの推定結果は、母平均をよりよく近似していることが確認できます。 対して単純なモデルは、依然として母平均から乖離しています。\nこのようなモデルの複雑さをめぐる問題は、Bias-Variance トレードオフと呼ばれてきました。 大規模事例を用いることができるのであれば、複雑なモデルが母平均を上手く近似できます。 事例数が少ない場合、複雑なモデルは母平均から大きく乖離してしまう可能性が高く、「妥協的に」単純なモデルを推定することが現実的です。\n応用上の問題は、適切なモデルの複雑さについて、理論的な示唆が限られていることです。 次の章では、複雑なモデルであったとしても適切に推定できる手法である、LASSOを紹介します。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>母集団上でのOLS</span>"
    ]
  },
  {
    "objectID": "PopulationOLS.html#rによる実践例",
    "href": "PopulationOLS.html#rによる実践例",
    "title": "4  母集団上でのOLS",
    "section": "4.4 Rによる実践例",
    "text": "4.4 Rによる実践例\n\n以下のパッケージを使用\n\nreadr (tidyverseに同梱): データの読み込み\nestimatr: OLS推定 \\(+\\) 頑健な信頼区間の計算\ndotwhisker: 信頼区間の可視化\n\n\nデータを取得します。\n\nData = readr::read_csv(\"Public.csv\") # データ読み込み\n\nlm_robust関数を用いてOLSを推定します。\n\nOLS = estimatr::lm_robust(\n  Price ~ Size + Tenure + StationDistance,\n  Data,\n  alpha = 0.05 # 95%信頼区間を計算\n  ) # OLS\n\nOLS\n\n                  Estimate Std. Error   t value      Pr(&gt;|t|)   CI Lower\n(Intercept)     19.7205516 0.62170089  31.72032 3.664958e-205 18.5018088\nSize             1.0199061 0.02020485  50.47828  0.000000e+00  0.9802978\nTenure          -0.6391756 0.01919741 -33.29488 3.014245e-224 -0.6768090\nStationDistance -1.3850527 0.06684087 -20.72165  2.316510e-92 -1.5160833\n                  CI Upper   DF\n(Intercept)     20.9392944 6374\nSize             1.0595144 6374\nTenure          -0.6015422 6374\nStationDistance -1.2540221 6374\n\n\nEstimateが\\(\\beta\\)の推定値を、CI Lower/CI Upperが信頼区間の下限/上限を示します。 例えば、推定された線型モデルにおいて、Size(部屋の広さ)と平均取引価格は正の関係性があります。 さらに信頼区間は \\([18.5,20.9]\\) なので、Population OLSにおいても正の関係性があることを強く示唆しています。\n上記の推定結果は、dotwhisker内のdwplot関数を用いて可視化できます。\n\n\n\n\n\n\n\n\n\n各点が推定値、横棒が信頼区間を図示しています。\n\n\n\n\nAngrist, Joshua D, and Jörn-Steffen Pischke. 2009. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton university press.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of Agnostic Statistics. Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>母集団上でのOLS</span>"
    ]
  },
  {
    "objectID": "PopulationOLS.html#footnotes",
    "href": "PopulationOLS.html#footnotes",
    "title": "4  母集団上でのOLS",
    "section": "",
    "text": "多重共線性の発生など、例外的なケースは存在します。↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>母集団上でのOLS</span>"
    ]
  },
  {
    "objectID": "LASSO.html",
    "href": "LASSO.html",
    "title": "5  LASSO",
    "section": "",
    "text": "5.1 推定方法\n複雑なモデルを適切に推定する方法として、LASSO (Tibshirani 1996) を紹介します。 LASSOは、罰則付き回帰と呼ばれる枠組みの一つの手法です1。 OLSと同様に線型予測モデルを推定しますが、データへの当てはまりだけでなく、モデルの複雑性も抑制することも目指します。\n例として、以下のモデルの推定を目指します。\\[\\beta_1 Size\\times 板橋区ダミー+.. + \\beta_6 Size^6\\times 板橋区ダミー\\] \\[+..+\\beta_{132} Size\\times 中央区ダミー+.. +\\beta_{138} Size^6\\times 中央区ダミー\\]\n合計138個のパラメタがあり、事例数次第では、OLSによる推定は困難です。 これはOLS推定が、以下を最小化するように\\(\\beta\\)を推定しているためです。 \\[(Y - モデル)^2 のデータ上の平均\\] \\(\\beta\\)の数が多いと、データへの不適合度 \\((Y - モデル)^2\\) をいくらでも低下させられるため、複雑なモデルを推定するとデータへの過剰な適合 (母平均からの乖離)を引き起こします。\nLASSO推定では、\\(\\beta\\) の値を以下を最小化するように決定します。\n\\(\\lambda\\) に応じて、予測モデルがどのように変化するのか考えてみます。 \\(\\lambda\\) を変化させることで、予測モデルは、単純平均と複雑なモデルのOLSの間で変化することになります。 \\(\\lambda=0\\) であれば、OLSと全く同じモデルを推定します。 よって複雑な線型モデルを推定した場合は、データ上の平均値に近いモデルとなります。 \\(\\lambda\\) を非常に大きい値を設定した場合、\\(\\beta_1=\\beta_2=..= 0\\) となります。 この場合は\\(\\beta_0\\)をデータに当てはまるように推定することになります。\n以下の数値例は、200事例からなるデータについて、LASSOにより以下のモデルを推定しました \\[\\beta_0 + \\beta_1Size+..+\\beta_6Size^6\\]\n\\(\\lambda\\)については、0,0.005,0.05、および赤池情報基準により選ばれた値 (Taddy 2017) を使用した結果を図示しています。\n母平均を青線、LASSOによる推定結果を赤線で示しています。\n\\(\\lambda=0\\) に比べると、\\(\\lambda\\) の値が大きくなるにつれ、モデルが単純な曲線に近づいていることが確認できます。 また Taddy (2017) に基づいて設定された\\(\\lambda\\) のもとでは、かなり単純化されたモデルが推定されたことも確認できます。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LASSO</span>"
    ]
  },
  {
    "objectID": "LASSO.html#推定方法",
    "href": "LASSO.html#推定方法",
    "title": "5  LASSO",
    "section": "",
    "text": "定義 LASSO\n\n\n\n\\[(Y - 予測値)^2 のデータ上の平均\\] \\[+ \\underbrace{\\lambda}_{Tunning\\ Parameter}\\times (\\beta_1の絶対値 +..)\\] \\(\\lambda\\)は、データへの当てはまりではなく、モデルの予測性能を高めるように決定する。 具体的には、交差検証を用いる方法(Tibshirani 1996)、情報基準などの理論的な評価指標を用いる方法などがある (Belloni, Chernozhukov, and Hansen 2014; Taddy 2017) 。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n練習問題\n\n\n\n\\(\\lambda\\) は、\\(\\beta\\) と異なり、データへの当てはまりを最大化するように決定できません。 なぜでしょうか？\n\n\n\n5.1.1 事例数の拡大\n推定結果は、一般に事例数に強く影響を受けます。 特にLASSOなどの機械学習の方法においては、データの特徴により強く依存します。\n以下の数値例では、事例数を100事例から1000事例まで増やし、 \\(Y\\sim Size + .. + Size^{10}\\) をLASSOで推定しています (\\(\\lambda\\) は Taddy (2017) の方法で設定しています)。\n\n\n\n\n\n\n\n\n\n事例数の増加とともに、モデルの複雑性が、「自動調整」されていることが確認できます。 100事例では、単純平均値が推定されており、極めて単純なモデルが採用されています。 事例が増えると、モデルの傾きに加えて、「曲がり方」も変化しており、モデルが複雑化しています。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LASSO</span>"
    ]
  },
  {
    "objectID": "LASSO.html#信頼区間",
    "href": "LASSO.html#信頼区間",
    "title": "5  LASSO",
    "section": "5.2 信頼区間",
    "text": "5.2 信頼区間\nLASSOによって推定されたパラメタについて、信頼区間を計算する方法は盛んに議論されているものの、筆者の知る限り、現状確立された方法は存在しません2。\nこれはLASSO以外の「機械学習」の手法についても同様です。 一般にデータと推定値との関係性は、伝統的な推定方法と比べて、機械学習の方が複雑になります。 このため、理論的な関係性を導くのが難しく、信頼区間の計算方法の確率が困難となっています。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LASSO</span>"
    ]
  },
  {
    "objectID": "LASSO.html#rによる実践例",
    "href": "LASSO.html#rによる実践例",
    "title": "5  LASSO",
    "section": "5.3 Rによる実践例",
    "text": "5.3 Rによる実践例\n\n以下のパッケージを使用\n\nreadr (tidyverseに同梱): データの読み込み\ngamlr: LASSO\n\n\n\nData = readr::read_csv(\"Public.csv\") # データ読み込み\n\nX = model.matrix(\n  ~ 0  +\n    (Size + Tenure + StationDistance + District)**2 + # 交差項\n    I(Size^2) + I(Tenure^2) + I(StationDistance^2), # 二乗項\n  Data\n) # X の作成\n\nX = scale(X) # 標準化\n\ncolnames(X) # Xに格納されている変数の確認\n\n [1] \"Size\"                             \"Tenure\"                          \n [3] \"StationDistance\"                  \"District世田谷区\"                \n [5] \"District中央区\"                   \"District中野区\"                  \n [7] \"District北区\"                     \"District千代田区\"                \n [9] \"District台東区\"                   \"District品川区\"                  \n[11] \"District大田区\"                   \"District文京区\"                  \n[13] \"District新宿区\"                   \"District杉並区\"                  \n[15] \"District板橋区\"                   \"District江戸川区\"                \n[17] \"District江東区\"                   \"District渋谷区\"                  \n[19] \"District港区\"                     \"District目黒区\"                  \n[21] \"District練馬区\"                   \"District荒川区\"                  \n[23] \"District葛飾区\"                   \"District豊島区\"                  \n[25] \"District足立区\"                   \"District墨田区\"                  \n[27] \"I(Size^2)\"                        \"I(Tenure^2)\"                     \n[29] \"I(StationDistance^2)\"             \"Size:Tenure\"                     \n[31] \"Size:StationDistance\"             \"Size:District中央区\"             \n[33] \"Size:District中野区\"              \"Size:District北区\"               \n[35] \"Size:District千代田区\"            \"Size:District台東区\"             \n[37] \"Size:District品川区\"              \"Size:District大田区\"             \n[39] \"Size:District文京区\"              \"Size:District新宿区\"             \n[41] \"Size:District杉並区\"              \"Size:District板橋区\"             \n[43] \"Size:District江戸川区\"            \"Size:District江東区\"             \n[45] \"Size:District渋谷区\"              \"Size:District港区\"               \n[47] \"Size:District目黒区\"              \"Size:District練馬区\"             \n[49] \"Size:District荒川区\"              \"Size:District葛飾区\"             \n[51] \"Size:District豊島区\"              \"Size:District足立区\"             \n[53] \"Size:District墨田区\"              \"Tenure:StationDistance\"          \n[55] \"Tenure:District中央区\"            \"Tenure:District中野区\"           \n[57] \"Tenure:District北区\"              \"Tenure:District千代田区\"         \n[59] \"Tenure:District台東区\"            \"Tenure:District品川区\"           \n[61] \"Tenure:District大田区\"            \"Tenure:District文京区\"           \n[63] \"Tenure:District新宿区\"            \"Tenure:District杉並区\"           \n[65] \"Tenure:District板橋区\"            \"Tenure:District江戸川区\"         \n[67] \"Tenure:District江東区\"            \"Tenure:District渋谷区\"           \n[69] \"Tenure:District港区\"              \"Tenure:District目黒区\"           \n[71] \"Tenure:District練馬区\"            \"Tenure:District荒川区\"           \n[73] \"Tenure:District葛飾区\"            \"Tenure:District豊島区\"           \n[75] \"Tenure:District足立区\"            \"Tenure:District墨田区\"           \n[77] \"StationDistance:District中央区\"   \"StationDistance:District中野区\"  \n[79] \"StationDistance:District北区\"     \"StationDistance:District千代田区\"\n[81] \"StationDistance:District台東区\"   \"StationDistance:District品川区\"  \n[83] \"StationDistance:District大田区\"   \"StationDistance:District文京区\"  \n[85] \"StationDistance:District新宿区\"   \"StationDistance:District杉並区\"  \n[87] \"StationDistance:District板橋区\"   \"StationDistance:District江戸川区\"\n[89] \"StationDistance:District江東区\"   \"StationDistance:District渋谷区\"  \n[91] \"StationDistance:District港区\"     \"StationDistance:District目黒区\"  \n[93] \"StationDistance:District練馬区\"   \"StationDistance:District荒川区\"  \n[95] \"StationDistance:District葛飾区\"   \"StationDistance:District豊島区\"  \n[97] \"StationDistance:District足立区\"   \"StationDistance:District墨田区\"  \n\n\n合計101個のパラメタ推定を目指します。\ngamlrパッケージ内のgamlr関数を用いてLASSO推定をします。\n\nLASSO = gamlr::gamlr(\n  y = Data$Price, # Yの指定\n  x = X\n) # LASSO推定\n\n推定された値は、以下で示します。 “.”は、厳密に0であることを意味しています。\n\ncoef(LASSO) # 推定値\n\n99 x 1 sparse Matrix of class \"dgCMatrix\"\n                                       seg100\nintercept                        42.705221072\nSize                             22.085283332\nTenure                           -0.949260232\nStationDistance                   .          \nDistrict世田谷区                  0.885399681\nDistrict中央区                    .          \nDistrict中野区                    .          \nDistrict北区                      .          \nDistrict千代田区                  .          \nDistrict台東区                    .          \nDistrict品川区                    .          \nDistrict大田区                    .          \nDistrict文京区                    .          \nDistrict新宿区                    .          \nDistrict杉並区                    .          \nDistrict板橋区                    .          \nDistrict江戸川区                  .          \nDistrict江東区                    .          \nDistrict渋谷区                    .          \nDistrict港区                      .          \nDistrict目黒区                    0.145531303\nDistrict練馬区                    .          \nDistrict荒川区                    .          \nDistrict葛飾区                    .          \nDistrict豊島区                    .          \nDistrict足立区                    .          \nDistrict墨田区                    .          \nI(Size^2)                         5.098523148\nI(Tenure^2)                       0.764221115\nI(StationDistance^2)              .          \nSize:Tenure                      -9.504525202\nSize:StationDistance             -3.840468008\nSize:District中央区               2.207591886\nSize:District中野区               .          \nSize:District北区                -0.947403269\nSize:District千代田区             4.536157097\nSize:District台東区               .          \nSize:District品川区               1.556391250\nSize:District大田区              -0.999011985\nSize:District文京区               1.658065622\nSize:District新宿区               2.554318454\nSize:District杉並区               0.092031271\nSize:District板橋区              -2.322997154\nSize:District江戸川区            -2.265860521\nSize:District江東区              -0.573435169\nSize:District渋谷区               5.671810968\nSize:District港区                12.715565955\nSize:District目黒区               2.106822926\nSize:District練馬区              -1.492913960\nSize:District荒川区              -1.064818067\nSize:District葛飾区              -2.532339361\nSize:District豊島区               0.818647790\nSize:District足立区              -3.474390892\nSize:District墨田区              -0.677389749\nTenure:StationDistance            0.285838918\nTenure:District中央区             .          \nTenure:District中野区             .          \nTenure:District北区               .          \nTenure:District千代田区          -1.110147900\nTenure:District台東区             0.400329988\nTenure:District品川区            -0.002439637\nTenure:District大田区             .          \nTenure:District文京区             .          \nTenure:District新宿区            -0.219994692\nTenure:District杉並区             .          \nTenure:District板橋区             .          \nTenure:District江戸川区           .          \nTenure:District江東区             .          \nTenure:District渋谷区            -1.158716799\nTenure:District港区              -2.258199491\nTenure:District目黒区             .          \nTenure:District練馬区             .          \nTenure:District荒川区             .          \nTenure:District葛飾区             .          \nTenure:District豊島区             .          \nTenure:District足立区             .          \nTenure:District墨田区             .          \nStationDistance:District中央区    .          \nStationDistance:District中野区    0.006454483\nStationDistance:District北区      .          \nStationDistance:District千代田区 -0.259898503\nStationDistance:District台東区    .          \nStationDistance:District品川区    .          \nStationDistance:District大田区    .          \nStationDistance:District文京区    .          \nStationDistance:District新宿区    0.099488399\nStationDistance:District杉並区    .          \nStationDistance:District板橋区    .          \nStationDistance:District江戸川区  .          \nStationDistance:District江東区    .          \nStationDistance:District渋谷区    .          \nStationDistance:District港区     -2.157523041\nStationDistance:District目黒区    0.217234860\nStationDistance:District練馬区    .          \nStationDistance:District荒川区    .          \nStationDistance:District葛飾区    .          \nStationDistance:District豊島区    .          \nStationDistance:District足立区    .          \nStationDistance:District墨田区    .          \n\n\n\n\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. “Inference on Treatment Effects After Selection Among High-Dimensional Controls.” Review of Economic Studies 81 (2): 608–50.\n\n\nChernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015. “Valid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.” Annu. Rev. Econ. 7 (1): 649–88.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2021. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nKuchibhotla, Arun K, John E Kolassa, and Todd A Kuffner. 2022. “Post-Selection Inference.” Annual Review of Statistics and Its Application 9 (1): 505–27.\n\n\nTaddy, Matt. 2017. “One-Step Estimator Paths for Concave Regularization.” Journal of Computational and Graphical Statistics 26 (3): 525–36.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society Series B: Statistical Methodology 58 (1): 267–88.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LASSO</span>"
    ]
  },
  {
    "objectID": "LASSO.html#footnotes",
    "href": "LASSO.html#footnotes",
    "title": "5  LASSO",
    "section": "",
    "text": "他の手法として、RidgeやBest subset selectionなどがあります。詳細は、James et al. (2021) などを参照ください。↩︎\n具体的な議論については、Chernozhukov, Hansen, and Spindler (2015); Kuchibhotla, Kolassa, and Kuffner (2022) などを参照↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LASSO</span>"
    ]
  },
  {
    "objectID": "prediction.html",
    "href": "prediction.html",
    "title": "6  予測分析への応用",
    "section": "",
    "text": "6.1 推定目標\n母平均のモデルの有力な応用例として、\\(Y\\)の予測問題への活用があります。 予測を目的とする研究では、観察できる変数 \\(X\\) から意思決定に必要だが欠損している情報 \\(Y\\) を予測するモデル（予測モデル）の推定を目指します。\n実例は、以下があげられます。\n統計学や機械学習においては、もし\\(Y,X\\) が共に観察できるデータを活用できるのであれば、予測モデルの推定方法がある程度確立されています。\n本章では、データと同じ母集団から新たに抽出された事例を、予測するモデルの推定を目指します。 予測性能は、新たな事例が抽出される母集団における平均二乗誤差で測定します \\[母集団における平均二乗誤差 = (Y - 予測値)^2 の母集団における平均\\] 母集団における平均二乗誤差は、直接計算することは不可能です。 ただし Section 6.2 で紹介する通り、推定することは可能です。\n本説では、実際に推定する前に、予測モデルが持つ基本的な性質を確認します。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>予測分析への応用</span>"
    ]
  },
  {
    "objectID": "prediction.html#推定目標",
    "href": "prediction.html#推定目標",
    "title": "6  予測分析への応用",
    "section": "",
    "text": "6.1.1 完璧な予測は不可能\n予測研究における究極的な目標の一つは、 \\(Y\\) を完璧に予測するモデルの推定です。 しかしながら多くの応用で、この目標には到達することができません。 予測モデルは、ある\\(X\\)の組み合わせについて、一つの予測値のみを出力します。 このため母集団において、同じ\\(X\\) 内で\\(Y\\) の値にばらつきがあれば、予測が外れる事例は必ず存在します。\n完璧な予測には、\\(Y\\)の全ての決定要因を\\(X\\)として観察し、\\(X\\)内での個人差をなくす必要があります。 しかしながら人間行動や社会的な事象などの社会的変数の決定要因は無数に存在し、その多くは観察が難しいと考えられます。 結果、社会的変数について、完璧な予測は不可能と考えられます。\n\n\n6.1.2 理想の予測モデル\n最も高い予測性能 (母集団における平均二乗誤差が最小化)を達成するモデルは、母平均であることが証明できます。 このため予測研究においては、データから推定した予測モデルを理想の予想モデルである母平均に極力近づけることが、実質的な目標となります。 具体的には以下を目指します \\[\\underbrace{X内でのYの母平均}_{理想の予測モデル}\\simeq 予測モデル\\] OLSやLASSOは、母平均のモデルを指定しており、理想の予測モデル推定を目指すための有力な方法であると考えられます。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>予測分析への応用</span>"
    ]
  },
  {
    "objectID": "prediction.html#sec-Estimate_MSE",
    "href": "prediction.html#sec-Estimate_MSE",
    "title": "6  予測分析への応用",
    "section": "6.2 予測性能の測定",
    "text": "6.2 予測性能の測定\n予測を目指す分析では、推定された予測モデルの性能を評価することが重要となります。 OLSやLASSOにより推定されたモデルの性質について、多くの理論研究が蓄積されています。 しかしながら現状、幅広いデータや状況において、一貫して高い予測性能を生み出す方法は存在しません。 また実際の予測力を理論のみに基づいて、算出することは困難です。 このため複数の予測モデルを”試作”し、データを用いてその性能を比較することが求められます。\n最もシンプルな評価方法は、サンプル分割です。\n\n\n\n\n\n\nサンプル分割法の定義\n\n\n\n\nデータをランダムに訓練データとテストデータに分割する\n\n\n訓練/テスト間での事例数の比率については、8対2や95対5が(経験則として)推奨されることが多い。\n\n\n訓練データのみでモデルを試作する\nテストデータへの当てはまりを(平均二乗誤差などで)評価する\n\n\n\n実際の取引データに適用した結果は以下です。 6378事例のうち、ランダムに選んだ半分を訓練、残り半分をテストに用いました。 テストした推定方法は以下です。\n\nOLS: 取引価格 ~ 部屋の広さ + 立地 + 取引年\nOLS (含む交差項 + 高次項): 取引価格 ~ 部屋の広さ + 立地 + 取引年 + 交差項 + 高次項(６次まで)\nLASSO (含む交差項 + 高次項): 取引価格 ~ 部屋の広さ + 立地 + 取引年 + 交差項 + 高次項(６次まで)\n\n推定された予測モデルの評価結果は以下となりました。\n\n\n\n\n\n\n\n\nOLS\nOLS| 交差項 + 高次項\nLASSO\n\n\n\n\n369\n316\n303\n\n\n\n\n\n\n\nLASSOが最も予測性能が高く、単純なOLSと比較し、0.179 パーセントほど平均二乗誤差を削減しています。 対して、交差項と高次項を追加したOLSとLASSOを比較した場合、0.041 パーセントほどの改善にとどまります。 これはモデルの複雑さに比べて、事例数が多く、LASSOによるモデル単純化の恩恵が限定的であることを示しています。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>予測分析への応用</span>"
    ]
  },
  {
    "objectID": "prediction.html#rによる実践例",
    "href": "prediction.html#rによる実践例",
    "title": "6  予測分析への応用",
    "section": "6.3 Rによる実践例",
    "text": "6.3 Rによる実践例\n\n以下のパッケージを使用\n\nreadr (tidyverseに同梱): データの読み込み\ngamlr: LASSO\n\n\n\n6.3.1 準備\nデータとその事例数を取得し、データをランダムに分割します。\n\nset.seed(111)\n\nData = readr::read_csv(\"Public.csv\") # データ読み込み\n\nX = model.matrix(\n  ~ 0  +\n    (Size + Tenure + StationDistance + District)**2 + # 交差項\n    I(Size^2) + I(Tenure^2) + I(StationDistance^2), # 二乗項\n  Data\n) # X の作成\n\nX = scale(X) # 標準化\n\nN = nrow(Data) # 事例数の取得\n\nGroup = sample(\n  1:2,\n  N, # 事例数\n  replace = TRUE, # 復元抽出を指定\n  prob = c(0.8,0.2) # \"1\"が8割、\"2\"が２割\n) # サンプル分割のために1または2をランダムに発生\n\nlm関数を用いてOLS, gamlr関数を用いてLASSO推定をします。 またLASSO推定は、複雑なモデルを推定に利点を持つため、交差項と二乗項までを導入したモデルも推定しています。\n\nLASSO = gamlr::gamlr(\n  y = Data$Price[Group == 1],\n  x = X[Group == 1,]) # LASSO\n\nOLS = lm(\n  Price ~ Size + Tenure + StationDistance + District,\n  Data,\n  subset = Group == 1\n)\n\nOLS_Long = lm(\n  Price ~ +\n    (Size + Tenure + StationDistance + District)**2 +\n    I(Size^2) + I(Tenure^2) + I(StationDistance^2),\n  Data,\n  subset = Group == 1\n)\n\nHatLASSO = predict(LASSO,X)\nHatOLS = predict(OLS,Data)\nHatOLS_Long = predict(OLS_Long,Data)\n\nmean((Data$Price - HatLASSO)[Group == 2]^2)\n\n[1] 154.7488\n\nmean((Data$Price - HatOLS)[Group == 2]^2)\n\n[1] 265.1663\n\nmean((Data$Price - HatOLS_Long)[Group == 2]^2)\n\n[1] 149.513",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>予測分析への応用</span>"
    ]
  },
  {
    "objectID": "prediction.html#まとめ",
    "href": "prediction.html#まとめ",
    "title": "6  予測分析への応用",
    "section": "6.4 まとめ",
    "text": "6.4 まとめ\n典型的な予測問題においては、データを用いた予測モデルの推定と評価、という二つの作業が求められます。 一般にこの作業を同じデータで行うと多くの問題が発生します。 本章では、データをランダムに２分割し、片方のデータで予測を、残りのデータで評価を行う方法を紹介しました。\n評価値としては平均二乗誤差を用いました。 現在、他の評価方法も盛んに研究されています (Angelopoulos, Bates, et al. (2023) などを参照してください)。\n\n\n\n\nAngelopoulos, Anastasios N, Stephen Bates, et al. 2023. “Conformal Prediction: A Gentle Introduction.” Foundations and Trends in Machine Learning 16 (4): 494–591.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>予測分析への応用</span>"
    ]
  },
  {
    "objectID": "tree.html",
    "href": "tree.html",
    "title": "7  回帰木モデル",
    "section": "",
    "text": "7.1 伝統的な方法\n本章では線型モデルの有力な代替案である、回帰木モデルを紹介します。 標準的な回帰木は、”サブグループ平均値”を母平均の推定値とします。\n回帰木モデルにおける最大の論点は、サブグループをどのように定義するのか、にあります。 伝統的には、研究者が背景知識などを用いて定式化してきました。 近年では、サブグループの定義もデータ主導で行う方法が注目されています。 特にモデル集計と呼ばれる手法も用いることで、母平均の近似精度を大きく改善することが期待されます。\n例えば、\\(X=\\) [部屋の広さ]、 \\(Y=\\) 取引価格、について、母平均 \\(E[Y\\mid X]\\) を推定する際に、サブグループ \\(\\{\\) 部屋の広さが60以上, 部屋の広さが60以下 \\(\\}\\) に分け、各サブグループごとに平均取引価格を計算し推定とすることができます。\nこのようなモデルは、以下のように樹形図として表現できます。\n一番上のボックスには、データ全体の平均取引価格 (43) とデータ全体に占める事例数の割合 (100 \\(\\%\\))を示しています。 下のボックスは、各サブグループの平均取引価格と事例数の割合を示しています。 部屋の広さが60以下であれば”yes”、以上であれば”no”です。 例えば、60以下のサブグループにおけう平均取引価格は31、事例数の割合は68 \\(\\%\\) となります。\nまた散布図に示すと以下のように、“階段状”のモデルとなります。 赤線が回帰モデル、青線は比較対象としてOLS (\\(Price\\sim\\beta_0 + \\beta_1\\times Size\\)) を示しています。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>回帰木モデル</span>"
    ]
  },
  {
    "objectID": "tree.html#データ主導の方法",
    "href": "tree.html#データ主導の方法",
    "title": "7  回帰木モデル",
    "section": "7.2 データ主導の方法",
    "text": "7.2 データ主導の方法\n伝統的な方法の応用上の問題は、サブグループの定義を研究者が行う必要があることです。 研究課題によっては、このような研究者主導のサブグループ分けは困難です。\nこの問題について、データ主導のサブグループ分けが提案されています。 最も代表的な方法は、データへの当てはまりの良さ を最大にするようにサブグループを定義する方法です。 この場合は、最大分割回数や各サブグループの事例数の下限値を設定した後、二乗誤差の平均値を最小化するようにサブグループを定義する方法が一般的です。\n以下ではrpart関数を用いて、最大２分割、最小事例数を50として、回帰木を推定しました。 \\(Y\\) は取引価格、 \\(X=[Size]\\) です。\n\nModel = rpart::rpart(\n    Price ~ Size, \n    data = Data,\n    control = rpart::rpart.control(\n      maxdepth = 2,\n      minbucket = 50,\n      minsplit = 1\n      )\n    ) # 回帰木の推定\n\nrpart.plot::rpart.plot(Model) # 可視化\n\n\n\n\n\n\n\n\nrpart関数は、「貪欲なアルゴリズム」を用いて、2グループへの分割を繰り返します。 一番最初の分割では、Sizeが48以上か否かでサブグループが定義されました。 これは48以上か否かで分割しサブグループ平均を計算するモデルが、最もデータへの当てはまりが良いためです。\n48以下の物件については、33以上か否かで2回目の分割が行われました。 この理由は、1回目の分割と同様に、33以上か否かで分割したモデルのデータへの当てはまりが良いためです。 同様に48以上については、78以上か否かで分割されます。\n回帰木は、複数の\\(X\\)を用いたモデルも生成できます。 以下では、Sizeに加え、築年数 (Tenure) および物件の立地 (LargeDistrict = [中心6区、その他]) も加えたモデルを推定しています。 すなわち \\(X=[Size,Tenure,LargeDistrict]\\) となります。\n\nModel = rpart::rpart(\n    Price ~ Size + Tenure + LargeDistrict, \n    data = Data,\n    control = rpart::rpart.control(\n      maxdepth = 2,\n      minbucket = 50,\n      minsplit = 1\n      )\n    ) # 回帰木の推定\n\nrpart.plot::rpart.plot(Model) # 可視化\n\n\n\n\n\n\n\n\n結果、Sizeが48以上の物件については、立地が中心6区か否かで２回目の分割が行われました。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>回帰木モデル</span>"
    ]
  },
  {
    "objectID": "tree.html#過剰適合への対処",
    "href": "tree.html#過剰適合への対処",
    "title": "7  回帰木モデル",
    "section": "7.3 過剰適合への対処",
    "text": "7.3 過剰適合への対処\n回帰木における複雑性は、最大分割回数や各サブグループの事例数の下限値などに操作できます。 最大分割回数を増やし、事例数の下限を減らせば、モデルはより複雑化します。 複雑なモデルは、事例数が十分にあれば、母平均の特徴をよりよく捉えることができます。 一方でOLSと同様に、複雑な回帰木モデルを推定すると、データへの適合度は高まる一方で、母平均からは乖離する傾向が生じます。\nこのような問題に対しては、LASSO同様に、モデルを適切に単純化する方法が考えられます。 代表的な方法としては、「剪定 (Pruning)」などが挙げられます。 詳細は、James et al. (2021) などを参照ください。\n次節では、母平均の近似を目指す場合により有力な方法である、モデル集計を紹介します。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>回帰木モデル</span>"
    ]
  },
  {
    "objectID": "tree.html#rによる実践例",
    "href": "tree.html#rによる実践例",
    "title": "7  回帰木モデル",
    "section": "7.4 Rによる実践例",
    "text": "7.4 Rによる実践例\n\n以下のパッケージを使用\n\nreadr (tidyverseに同梱): データの読み込み\nrpart (Rに同梱): 回帰木の推定\nrpart.plot : 回帰木の可視化\n\n\nデータを取得します。\n\nData = readr::read_csv(\"Public.csv\") # データ読み込み\n\nrpart関数を用いてOLSを推定します。\n\nTree = rpart::rpart(\n  Price ~ Size + Tenure + StationDistance + LargeDistrict, # Y ~ X\n  Data, # 使用するデータの指定\n  control = rpart::rpart.control(\n    maxdepth = 3, # 最大分割回数 = 2\n    minsplit = 50, # 50以下になったら分割を停止\n    minbucket = 50 # 50以下のサブグループを作らない\n  )\n  )\n\nTree\n\nn= 6378 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 6378 5935190.00  42.70522  \n   2) Size&lt; 47.5 3359  503187.50  26.41390  \n     4) Size&lt; 32.5 2458  147489.90  22.38031 *\n     5) Size&gt;=32.5 901  206606.60  37.41787  \n      10) Tenure&gt;=25.5 386   43399.57  27.53238 *\n      11) Tenure&lt; 25.5 515   97213.62  44.82718 *\n   3) Size&gt;=47.5 3019 3548597.00  60.83127  \n     6) LargeDistrict=その他 2430 1257966.00  52.28255  \n      12) Tenure&gt;=24.5 837  218600.20  38.45950 *\n      13) Tenure&lt; 24.5 1593  795403.00  59.54551 *\n     7) LargeDistrict=中心6区 589 1380391.00  96.10017  \n      14) Size&lt; 72.5 446  412038.60  81.80269 *\n      15) Size&gt;=72.5 143  592832.50 140.69230 *\n\n\nrpart.plot関数を用いて可視化します。\n\nrpart.plot::rpart.plot(Tree)\n\n\n\n\n\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2021. An Introduction to Statistical Learning. Vol. 112. Springer.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>回帰木モデル</span>"
    ]
  },
  {
    "objectID": "Reference.html",
    "href": "Reference.html",
    "title": "Reference",
    "section": "",
    "text": "Angelopoulos, Anastasios N, Stephen Bates, et al. 2023. “Conformal\nPrediction: A Gentle Introduction.” Foundations and\nTrends in Machine Learning 16 (4): 494–591.\n\n\nAngrist, Joshua D, and Jörn-Steffen Pischke. 2009. Mostly Harmless\nEconometrics: An Empiricist’s Companion. Princeton university\npress.\n\n\nAronow, Peter M, and Benjamin T Miller. 2019. Foundations of\nAgnostic Statistics. Cambridge University Press.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014.\n“Inference on Treatment Effects After Selection Among\nHigh-Dimensional Controls.” Review of Economic Studies\n81 (2): 608–50.\n\n\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler,\nand Vasilis Syrgkanis. 2024. “Applied Causal Inference Powered by\nML and AI.” arXiv Preprint arXiv:2403.02467.\n\n\nChernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015.\n“Valid Post-Selection and Post-Regularization Inference: An\nElementary, General Approach.” Annu. Rev. Econ. 7 (1):\n649–88.\n\n\nDing, Peng. 2024. “Linear Model and Extensions.” arXiv\nPreprint arXiv:2401.00649.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2021. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nKuchibhotla, Arun K, John E Kolassa, and Todd A Kuffner. 2022.\n“Post-Selection Inference.” Annual Review of Statistics\nand Its Application 9 (1): 505–27.\n\n\nLin, Hanti. 2024. “To Be a Frequentist\nor Bayesian? Five Positions in a\nSpectrum.” Harvard Data Science Review 6\n(3).\n\n\nTaddy, Matt. 2017. “One-Step Estimator Paths for Concave\nRegularization.” Journal of Computational and Graphical\nStatistics 26 (3): 525–36.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society Series\nB: Statistical Methodology 58 (1): 267–88.",
    "crumbs": [
      "Reference"
    ]
  }
]